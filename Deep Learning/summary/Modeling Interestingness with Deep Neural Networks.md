# Modeling Interestingness with Deep Neural Networks

[è®ºæ–‡åŸæ–‡]()

> ç”¨æˆ·uçœ‹æ¥æ–‡ç« aä¹‹åï¼Œä¸ºä»–æ¨èæ–‡ç« b

## æ¡†æ¶

![](res/128.jpg)

## Network Architecture

### Input Layer

æ¯ä¸€ä¸ªè¯one-hotç¼–ç +tri-letter(â€œ#dog#â€,->â€œ#doâ€, â€œdogâ€, and â€œog#â€.)ç›®çš„æ˜¯å¦‚æœone-hotçš„å­—å…¸åº“ä¸­æ²¡æœ‰å‡ºç°çš„ï¼Œå¯ä»¥é€šè¿‡tri-letteråŒºåˆ†

### Convolutional Layer

çª—å£å¤§å°ä¸º3ï¼Œå¯¹æ¯ä¸€ä¸ªè¯è¿›è¡Œå·ç§¯

![](res/129.jpg)

### Max-pooling Layer

å¯¹æ‰€æœ‰è¯ï¼Œä»æ¯ä¸€ä¸ªç»´åº¦è¿›è¡Œmax-poolingï¼Œä¸€ä¸ªç»´åº¦ç•™ä¸‹ä¸€ä¸ªæœ€å¤§çš„è¯ï¼ˆ300ç»´ï¼Œ300ä¸ªè¯ï¼‰

![](res/130.jpg)

### Fully-Connected Layers ğ¡ and ğ²

![](res/131.jpg)

## Training the DSSM

pair-wise rank loss

![](res/132.jpg)
![](res/133.jpg)
![](res/134.jpg)